\chapter{REVISÃO BIBLIOGRÁFICA}

\section{SISTEMAS DE MONITORAMENTO DE DESMATAMENTO DO INPE}
\label{rainforests:sec:monitoring}

Por mais de duas décadas, o Brasil vem utilizando imagens de satélites para realizar o monitoramento da Amazônia \cite{Monteiro2008}. Estes sistemas, desenvolvidos pelo INPE, tornaram o Brasil uma referência mundial na área \cite{Tollefson2012a}. \citeonline{Kintisch2007} afirma que esse sistema é motivo de admiração mundial por ser capaz de informar anualmente as estimativas de taxas de desmatamento na Amazônia, além de emitir alertas semanais para as autoridades pertinentes. Os principais sistemas utilizados pelo INPE na tarefa de monitorar o desmatamento são descritos nas seções a seguir.

\section{PRODES}
\label{rainforests:sec:prodes}
Em 1988, o Projeto de Monitoramento do Desmatamento na Amazônia Legal (PRODES) foi desenvolvido para fornecer informações sobre a dinâmica anual do desmatamento de cobertura florestal na Amazônia Legal. As estimativas geradas pelo PRODES são anuais devido à complexidade e ao detalhamento necessários para o cálculo da área de desmate. Essas estimativas se baseiam em mapeamento detalhado com um grande conjunto de imagens do tipo LANDSAT (ou equivalente), que cobrem a Amazônia com baixa frequência temporal (16 e 26 dias) e com resolução espacial entre 20 e 30 metros. Esses sensores são capazes de mapear desmatamentos cujas áreas sejam superiores a $6,25$ hectares \cite{Monteiro2008}.

Para realizar o cálculo da taxa de desmatamento as imagens são selecionadas de modo a obter a menor cobertura de nuvens possível, melhor visibilidade com uma adequada qualidade radiométrica\footnote{A resolução radiométrica é dada pelo número de valores digitais representando níveis de cinza, usados para expressar os dados coletados pelo sensor. Quanto maior o número de valores, maior é a resolução radiométrica} e com a data de aquisição das imagens próxima ao período de referência para o cálculo da taxa de desmatamento. Porém, considerando o histórico climatológico da Amazônia, a maioria das imagens não se apresentam livres de nuvens. Por isso é possível utilizar mais de uma imagem (inclusive de outros satélites) para compor as cenas. 

Após a seleção das imagens, a próxima etapa envolve transformar seus dados radiométricos em componentes de cena (vegetação, solo e sombra), utilizando o Modelo Linear de Mistura Espectral (MLME). As bandas 3, 4 e 5 do sensor TM são utilizadas para estimar a proporção dos componentes solo, vegetação e sombra para cada pixel, formando um sistema de equações lineares que pode ser solucionado pelo método dos mínimos quadrados ponderados. O resultado desse modelo linear é uma imagem-fração, onde se tem três bandas sintéticas com os valores proporcionais de vegetação, solo e sombra. A segmentação\footnote{Segmentação de imagem é uma técnica de agrupamentos de dados onde características espectrais semelhantes são agrupadas.} da imagem-fração é então realizada, ajustando-se os limiares de similaridades e de área. 

Um algoritmo de classificação não-supervisionado de agrupamentos de dados trata as imagens segmentadas, classificando-as de acordo com as classes definidas pelo banco de dados. Como resultado tem-se uma nova imagem \textit{raster} ou vetorial com as áreas desflorestadas. Então, um fotointerprete tem a tarefa de analisar os polígonos temáticos gerados, tomando a decisão se esses devem ser aceitos ou reclassificados. Uma vez essa imagem aceita, uma máscara de desmatamento contendo as áreas de corte raso já detectados é gerada. Essa máscara será utilizada para eliminar desmatamentos antigos, impedindo que sejam identificados novamente.

%%
\section{DETER}
\label{rainforests:sec:deter}
Devido ao tempo necessário para gerar os resultados e por observar apenas áreas de corte raso, o PRODES não pode ser utilizado como um sistema de prevenção. Portanto, a partir de 2004 o Sistema de Detecção de Desmatamento em Tempo Real (DETER) foi implementado para realizar o monitoramento contínuo do desmatamento e da degradação florestal. Esse sistema foi criado para atender ao Governo Federal no Plano de Ação para a Prevenção e Controle do Desmatamento na Amazônia Legal. O principal objetivo desse sistema é de fornecer informações sobre o local e a dimensão aproximada de ocorrências de mudanças na vegetação de modo a agilizar a fiscalização. 

As imagens utilizadas por esse sistema são obtidas pelo sensor MODIS (a bordo do satélite TERRA da NASA), que cobre a Amazônia a cada dia e meio. Essa alta resolução temporal reduz as limitações de observação impostas pela cobertura de nuvens da região. Com a máxima resolução espacial limitada em $250$ metros, as imagens desses sensores permitem a detecção de desmatamentos apenas para áreas maiores do que $25$ hectares. O objetivo do DETER é de fornecer indicadores para fiscalização a cada 15 dias, quando as condições de observação são favoráveis. Esse sistema observa diversos estágios de desmatamento para emitir seus alertas, como o de corte raso, degradação florestal de intensidade alta, média e baixa, sendo o último mais difícil devido a resolução das imagens do sensor MODIS \cite{Monteiro2008}.

A aquisição das imagens é feita de forma rápida, uma vez que que o DETER utiliza os produtos baseados em \textit{granules}\footnote{Granules são produtos gerados de uma área particular. Granules não cobrem todo o globo.} dos subconjuntos de resposta rápida da NASA. Esses dados encontram-se prontos para serem utilizados, pois já foram processados, disponibilizados em GeoTIFF, RGB-equivalente, no formato de 8-bits e geograficamente projetados. 

Essas imagens são então carregadas no Sistema de Processamento de Informações Geo-referenciadas (SPRING) para que outros processamentos sejam feitos. Nesta etapa o especialista necessita aplicar um modelo de mistura para separar o que é floresta, solo ou água (ou sombra). Essa etapa é feita selecionando-se certos \textit{pixels} com uma resposta espectral particular. Então, cada imagem é segmentada e classificada. Após a classificação das imagens, o especialista aplica as máscaras dos desflorestamentos anteriores e de hidrografia, com a finalidade de esconder os desmatamentos já conhecidos assim como outras características. 

Na última etapa, o especialista corrige os resultados da segmentação automática, pixel-a-pixel. As vezes é possível que as etapas de classificação e segmentação possam ser colocados de lado, pois o especialista pode extrair todas as informações baseando-se apenas em sua expertise olhando para as imagens do satélite, munido dos arquivos geográficos de desflorestamento e hidrografia.


\section{Ciência Cidadã}

Ciência cidadã é o termo usado para designar projetos no qual voluntários, muitos sem nenhum treinamento científico específico, efetuam ou gerenciam tarefas científicas, tais como a realização de observações, medições ou computação \cite{SoaresSant:2011:EnPoAt}. 

Através do voluntariado de pessoas ordinárias, conhecidas como cientistas cidadão, projetos científicos conseguem obter um quadro maior de colaboradores \cite{Cohn.2008}. Um fator importante em projetos grandes, agilizando o processo de aquisição e divulgação dos resultados. Segundo \citeonline{Silvertown2009}, o cientista cidadão é um voluntário que coleta ou processa dados como parte de uma investigação científica. Cientistas cidadão não são responsáveis, necessariamente, por analisar ou publicar artigos científicos, estes desempenham tarefas simples, mas de grande importância para a conclusão dos trabalhos científicos. Nas últimas décadas, projetos científicos baseados em ciência cidadã ganharam notoriedade, porém esta abordagem não é nova para a comunidade científica. 

Realizar pesquisas cientificas utilizando-se da colaboração de diversos indivíduos vem de tempos remotos. Para ilustrar, A Origem das Espécies é um exemplo de pesquisa científica realizada com a ajuda de diversos colaboradores já em 1830. \citeonline{darwin-origin-of-species-1859}, contou com a ajuda de mais de 2000 colaboradores entre esses, especialistas, biólogos e pesquisadores de diferentes áreas, tendo também a colaboração de cientistas de diferentes áreas.  O projeto \textit{\textbf{Darwin Correspondences}}\footnote{\url{http://www.darwinproject.ac.uk/}} reune mais de 7.500 das cartas que Darwin manteve com seus correspondentes durante sua pesquisa \cite{DarwinProject_Correspondents}. 

Os conteúdos destas cartas variavam de notações científicas sobre algumas espécies, o que requeria um aparato profissional, ou de apenas simples observações, que vieram a colaborar com teoria da evolução das espécies. Naquela época, as cartas demoravam meses para serem recebidas e lidas,  o processo de responder uma carta e obter um novo retorno da mesma pessoa, chegava a levar questões de anos para acontecer. Tudo isto devido a este meio de comunicação não ter as mesmas tecnologias atuais, tornando a tarefa de entregar uma carta hoje em dia simples e comum , um trabalho dificultoso e lento. Naquela época, os serviços de correspondências era feito por mensageiros a pé, a cavalo ou através de charretes, tendo návios à vela para correspondências pelos mares. \citeonline{Hyde1891} relembra que para a determinada época, estes serviços eram caros e não acessível para todos, o que dificultava ainda mais o compartilhamento de ideias. Em 1840, com a grande reforma britânica de postagem,  \textit{Penny Postage} \footnote{A reforma realizada pela \textit{Royal Mail} do Reino Unido cobrava apenas um \textit{Penny}, menor moeda do sistema monetário da época, para entregar as cartas indiferente da distância.}, houve uma maior difusão e uso dos serviços, elevando o envio de cartas de 82.500.000 para 169.000.000 em um ano, mais que o dobro.

\citeonline{Zimmer2011}, comenta que os resultados obtidos no recente trabalho de \citeonline{Silvertown2011} seria uma das formas que Darwin faria ciência hoje através da Internet. Neste trabalho, \citeonline{Silvertown2011} observa mudanças evolucionárias de um continente através de colaboradores que utilizaram um projeto de ciência cidadã moderno, Evolution MegaLab, iniciado em 2008 o projeto contava com colaboradores para enviar informações de duas espécies de caramujos, Cepaea nemoralis and C. hortensis, para realizar um estudo de evolução da espécie observando as diferentes cores de suas cascas.

Um projeto da universidade de Oxford irá investigar o envolvimento do público na ciência do século 19 e 21 \cite{conscicom2014}, receberá o financeamento de quase 2 milhões de libras para realizar seus estudos. Supõe-se que este estudo poderá entender e desenvolver novas ferramentas para trocar informações entre cientistas profissionais e legiões de voluntários \cite{Leicester2013}. 

O projeto considerado um dos primeiros de ciência cidadã moderno é o \textit{Christmas Bird Count}. Um projeto antigo e que ainda encontra-se em atividade, o Christmas Bird Count procura contar as diferentes espécies que existem na américa do Norte, suas eventuais mudanças de habitat, entre outras informações. Idealizado em 1900 por Frank Chapman, um famoso ornitólogo do Museu Americano de História Natural, como uma atividade alternativa ao evento de caça aos pássaros existente na época, Chapman publicou diversos livros com os resultados obtidos por este projeto com a ajuda de milhares de voluntários. Estes seguem diversas regras para conduzir a pesquisa durante os 20 dias em que as observações são feitas, de 14 de dezembro a 5 de janeiro de cada ano, só podem ser contabilizados os pássaros que forem avistados ou ouvidos em um diâmetro de 24-km, moradores próximos a estas áreas podem utilizar bebedouros para pássaros para atrair mais espécies e contabilizá-los \cite{Silvertown2009}. Em uma contagem recente milhares de observadores relataram mais de $63$ milhões de pássaros. 

Hoje, o centro de pesquisa que deu origem a este projeto é considerado um dos maiores centros especializados em ciência cidadã com diversos estudos em biologia. Cientistas do laboratório Cornell de Ornitologia universidade de Cornell, lídereres no estudo e conservação dos pássaros, rastreiam projetos que utilizam ciência cidadã para realizar seus estudos. Estes acreditam que trabalhar com cientistas cidadãos é um fenômeno em expansão em todo o mundo \cite{Cohn.2008}. Este laboratório conta com uma comunidade de aproximadamente 200 mil participantes de ciência cidadã.

Ainda há dúvidas se os projetos de ciência cidadã possam gerar resultados confiáveis, uma vez que muitos dos voluntários engajados nas atividades científicas não possuem conhecimento nem mesmo familiarização com as ferramentas de coleta. Esta é uma questão muito pertinente e recorrente do meio. Há evidências \cite{Silvertown2009,Silvertown2011,Cohn.2008} que estes dados produzidos por cidadãos comuns possam sim ser confiáveis. Para isto é necessário que algumas medidas sejam seguidas.

Para alguns tipos de projetos, \citeonline{Cohn.2008} defende que os voluntários devem possuir algum tipo de treinamento básico, para que os dados sejam coletados conforme o solicitados pelos cientistas, assim diretrizes devem ser definidas. Parte dessas diretrizes devem limitar o trabalho do voluntário, especificando um determinado foco de coleta, por exemplo. Esta especificação evita diversos ruídos nos dados e ao comparar a coleta feita entre os voluntários, os dados seguirão a mesma semântica, facilitando a verificação de erros. Há relatos que projetos anteriores baseados em ciência cidadã, possuiam resultados variados por causa destes, ao invés de dados exatos \cite{Cohn.2008}. Solicitar que os voluntários desempenhem trabalhos simples auxilia na exatidão dos dados. 

\cite{Silvertown2009} enumera três fatores cruciais para o aparecimento de projetos de ciência cidadã nos últimos anos. Primeiramente, a Internet como meio de disseminar informações e adquirir dados do público, assim como a tecnologia dos \textit{smartphones}, onde mais e mais aplicativos destes utilizam diversos sensores para coletar diferentes dados. Segundo fator se deve aos cientistas profissionais perceberem que voluntários são uma fonte sem custos de trabalho e habilidades pessoais como também de poder computacional, projetos que requerem adquirir diversos dados ao longo do globo, necessitam de ajuda, podendo ser de voluntários, para se obter sucesso. Terceiro fator, grandes fundadores de projetos científicos procuram beneficiar projetos que utilizam cientistas cidadãos em seus trabalhos.

\subsection{Ciência Cidadã Moderna e a Internet}

Recentemente, o número de projetos que se beneficiam de ciência cidadã está aumentando, cada dia há novos projetos surgindo. Estes projetos chamados de ciência cidadã moderno estão se tornando frequente por causa da accebilidade das tecnologias atuais, o que não requer aparatos especializados para realizar as pesquisas. Como mencionado anteriormente, as informações que \citeonline{Darwin1859} utilizou em suas pesquisas foram enviadas através de cartas que demoravam muitos meses. Hoje, com o avanço da internet, voluntários em diferentes partes do mundo podem fornecer diferentes tipos de dados a uma pesquisa. Seja por dispositivos móveis, que uma vez ligados a internet podem fornecer dados de qualquer lugar, ou então por meio de computadores. 

Na década de 80, a internet ainda era apenas um embrião e poucos tinham acesso. Existiam menos de 200,000 servidores espalhados no mundo. Até então não existiam páginas para serem navegadas, a principal forma de troca de mensagem era através de e-mail, criado em 1977 \cite{HistoryOfInternet:David,HistoryOfInternet:Anthony}, outros meios eram por telnet e IRC, apenas. Só no início da década de 90 que as primeiras páginas de internet foram criadas, após a definição do \textit{WWW}\footnote{World-Wide Web} criado por \citeonline{berners1992world}. A internet estava tornando-se popular, com seus aproximados 1 milhão de servidores e suas 50 páginas de internet.

\subsubsection{Computação Voluntária}

Com a popularização da internet, iniciou-se a aparição de projetos notáveis da ciência cidadã moderna. Os computadores desta época eram caros e possuiam pouco poder computacional, apenas grandes indústrias e universidades tinham acesso a máquinas de grande desempenho. Neste período, surgiram os primeiros projetos de computação voluntária, onde os voluntários doavam o tempo de processamento ocioso de suas máquinas a projetos que necessitavam de grande poder computacional para trabalhar em cima dos seus dados. O conceito desta forma de projeto era de dividir a grande massa de dados existente em pequenas porções que fossem possíveis para os voluntários efetuar downloads, visto que naquela época não havia internet de banda larga. A forma de conexão à internet ainda era discada e o modem mais rápido deste período era o de 56k\footnote{56 kilobits por segundo} \cite{AndersonCKLW02}.

No meio da década de 90, surgiu os projetos \textbf{\textit{GIMPS}}\footnote{\textit{\url{http://www.mersenne.org/}-Great Internet Mersenne Prime Search}} e \textbf{\textit{Distributed.net}}\footnote{http://www.distributed.net/}\cite{Anderson1999,AndersonCKLW02,Hayes1998}.

\subsubsection{Projetos de Computação Voluntária}

Alguns dos projetos que fizeram a computação voluntária uma realidad hoje são:

\textit{GIMPS} foi primeiro projeto de computação voluntária de grande porte a ser realizado, \textit{GIMPS}, tinha como objetivo encontrar números primos de Mersenne, nome dado em homenagem ao estudioso Marin Mersenne da teoria dos números. A formula destes números equivale  $ M_n = 2^n -1 $, onde $n$ é um número natural. O desafio de descobrir números de Mersenne está diretamente ligado ao fato destes números serem exponênciais, tendo assim milhares de dígitos em sua composição. Até 1996, início do projeto \textit{GIMPS}, apenas 34 números primos de Mersenne eram conhecidos, logo no primeiro ano do projeto foram descubertos mais dois  números, $ M_{1398269} $ e $M_{2976221}$. O primeiro número possui $852.365$ algoritmos, o segundo $1.814.262$, uma operação que seria impossível para ser realizada por uma pessoa. Ambos foram descobertos na primeira versão do software disponibilizado pelo projeto, sendo calculado por um computador Pentium 90 MHz e Pentium 100MHz, respectivamente. Hoje, há o conhecimento de 48 números de Mersenne, sendo o último número descoberto o $M_{57885161}$ com $17.425.170$ dígitos em 2013, desde o início do projeto foram descobertos 14 destes números \cite{Marsenne:Primes,Hayes1998}.

A \textit{Distributed.net}, lançado em 1997, tinha o principal objetivo de quebrar a criptografia gerada pela empresa RSA, para o desafio \textit{RSA Secret-Key Challenge} que correspondia a uma chave de 56-bit e possuia uma recompensa de $10.000,00$ dólares. O projeto criado por Earle Ady e Christopher G. Stach II, contava com a colaboração de mais de $300.000$ voluntários utilizando o tempo de seus computadores realizando Força Bruta\footnote{Força Bruta é a forma de tentar obter a a resposta de uma senha através de ínumeras tentativas.} em cima de parte do código disponibilizado para o desafio. Em 250 dias a chave foi descoberta, utilizando um poder computacional equivalente a 26 mil \textit{Pentium 200}. Outro desafio com uma chave de 64-bit também foi concluído após 4 anos e o prêmio pago. A empresa de segurança RSA, havia dito que para quebrar uma chave de 64-bit, seria necessário mais de 100 anos testando todas as possibilidades e combinações. Atualmente o projeto está focado em quebrar uma chave de 72-bit \cite{distributed.net:online,Distributed.net:wired}. 

Em 1999, \citeonline{AndersonCKLW02} iniciou um dos primeiros e bem sucedido projeto de ciência cidadã, o objetivo deste projeto era de encontrar vida inteligente no espaço, através da análise de sinais de rádios captadas do espaço, \textit{SETI@Home}. Porém para conseguir analizar esses sinais, o projeto contou com o uso de diferentes computares, todos espalhados pela internet formando um grande sistema distruído de processamento. Os voluntários que se cadastravam no site podiam fazer download de um aplicativo que só era ativado quado o computador estava em modo ocioso, o aplicativo recebia pacotes de sinais a serem analizados e no fim do processamento enviava os resultados obtidos ao servidor do projeto.


\subsection{A Chegada da Web 2.0}

\cite{Anderson1999}, a frente do projeto \textit{SETI@Home}, iniciou o desenvolvimento de uma nova ferramenta para diminuir as barreiras que ele havia encontrado ao longo do desenvolvimento de seu projeto, viabilizando assim novas iniciativas para utilizar computação voluntária de forma rápida e sem grandes conhecimentos de computação. Outra função da ferramenta, BOINC\footnote{Berkeley Open Infrastructure for Network Computing}, era de avaliar a exatidão e veracidade dos dados antes de enviá-los aos servidores dos projetos \cite{anderson2003public}. Esta ferramenta já foi utilizada por mais de 150 projetos, tendo atualmente 70 projetos online. Estes fatos só foram alcançados pela acessibilidade que novos projetos do tipo de computação voluntária tiveram com a criação da nova ferramenta e também pela visibilidade que a ferramenta deu aos projetos deste porte.

No fim da década de 90, \citeonline{dinucci1999fragmented} cunhou o termo \textit{Web 2.0} dizendo que a internet até então não tinha mostrado o seu real potencial. As páginas eram simplesmentes recursos estáticos onde a navegação era composta de uma simples requisição a este recurso e o recurso era então exibido na tela dos computadores da época. A revolução da \textit{Web 2.0} seria marcada pela interatividade do conteúdo, permitindo a qualquer pessoa utilizando de um computador ou dispositivo móvel, não mais carregar um simples recurso estático, mas sim o poder de interagir com este recurso, expressando ideias e adicionando novas informações a \textit{Web}. Diversas novas ferramentas foram criadas nesta nova era. Diferentes tipos de \textit{blogs}, \textit{wikis} e páginas de internet repletos de conteúdos dinâmicos.

Além da ferramenta \textit{BOINC} realizar verificações redundantes para melhorar a exatidão dos resultados, esta também era uma plataforma de pontuação. Na página da ferramenta existem diversas estatísticas dos projetos em andamento, estas estatísticas são atualizadas dinâmicamente conforme os resultados são submetidos pelos voluntários. Uma vez que os novos resultados são submetidos e aprovados, cada voluntário tem o valor da sua contribuição ao projeto calculado novamente. Tendo assim um sistema de creditos, que pontua a participação dos voluntários, destacando os que mais contribuem. Este é considerado o sistema de recompensa dos usuários, levando o projeto a ter cada vez mais voluntários contribuindo com a performance da sua máquina para ganhar notoriedade \cite{Anderson1999}.

\subsubsection{Projetos de Pensamento Voluntário}

Com a possibilidade de conteúdo dinâmico e a interação dos usuários, sugiram novos tipos de projetos que passaram a utilizar a capacidade cognitiva dos voluntários para analisar visualmente determinados dados e em função destes tomar ações. 

Um dos primeiros projetos de pensamento voluntário foi criado para detectar pequenas partículas de poeira interestelar coletada pela mssão \textit{Stardust} da NASA, lançado em 1999 \cite{Stardust:Mission}. Andrew Westpahl, o idealizador do projeto \textit{Stardust@Home} teve a ideia de utilizar a percepção dos usuários para substituir a falta de uma tecnologia de reconhecimento de padrão capaz de encontrar as partículas mínusculas coletada pela missão. Westpahl, estima que levaria mais de um século para sua equipe poder atingir o objetivo do projeto sem a ajuda dos voluntários. Para encontrar as partículas, foi disponibilizado no início do projeto em 2006, 1.6 milhões de imagens na página do projeto, estas imagens recriam a experiência que um cientista exerceria se estivesse analisando as amostras atrás de particulas através de um microscópio, estabelecendo uma melhor imagem ajustando o seu foco. Estas 1.6 milhões de imagens, foram feitas para simular esta função, chamado de ``filmes de foco'', onde diversas imagens foram feitas de um determinado ponto mas utilizando posições diferentes para se ter uma melhor nitidez da imagem ou não. É função do voluntário verificar se esta imagem esta bem focada, verificando a nitidez das imagens disponibilizadas e se há alguma particula presente em alguma destas imagens\cite{Hand2010}. Caso não exista nenhuma imagem focada, o voluntário deve informar através do site esta situação, assim como outras diveras situações que podem acontecer. Para conhecer essas determinadas situações, os voluntários necessitam realizar um treinamento antes de iniciar o seu trabalho de voluntário efetivamente, neste treinamento há dicas e orientações de como proceder. Este tipo de treinamento de qualificação é muito comum em projetos de ciência cidadã \cite{Silvertown2009,Anderson1999}.

Apesar do projeto \textit{Stardust@Home} possuir o sufixo \textit{@Home}, este não utiliza a ferramenta BOINC, idealizada por \citeonline{anderson2003public}. Este sufixo é apenas uma homenagem aos projetos de ciência cidadã, efetuado por estes. Contudo, \citeonline{Anderson1999} estudou o projeto e idealizou uma nova ferramenta para tornar projetos de pensamento voluntário mais comuns, assim como o uso da ferramenta BOINC. BOSSA\footnote{\url{http://boinc.berkeley.edu/trac/wiki/BossaIntro}} é um \textit{middleware} responsável por dividir o trabalho em tarefas menores e atribuí-las aos voluntários, realizando verificações do nível de acertividade das respostas dadas pelos voluntários através de redundância das tarefas. 

Em 2007, iniciou-se o projeto \textit{GalaxyZoo} com o objetivo de classificar imagens de galaxias. As imagens captadas por um telescópio robótico, \textit{Sloan Digital Sky Survey}\footnote{http://www.sdss.org/} era apresentadas para os voluntários e estes haviam de decidir se a imagem continha alguma galáxia, se houvesse, qual era a sua forma eliptica ou espiral, se fosse esta última, ainda havia uma última pergunta, qual o sentido da sua rotação\cite{Hand2010}. Pelo número de imagens que deveriam ser classificadas os cientistas envolvidos acreditavam que iria demorar mais de 2 anos para que todas as imagens fossem classificadas\cite{GalaxyZooAbout}. Porém, com apenas um dia de funcionamento, o \textit{GalaxyZoo} conseguiu reunir 35 mil voluntários que fizeram aproximadamente 1.5 milhões de classificações. \citeonline{Raddick2009b} compara o resultado obtido em um dia com a de um aluno de graduação que em uma semana conseguiu classificar apenas 50 mil galáxias. O projeto classificou perto de um milhão de galáxias utilizando-se de mais de 150 mil voluntários. Em sua segunda versão, o \textbf{GalaxyZoo} contou com mais de 200 mil voluntários para classificar de forma mais detalhada 300 mil galáxias previamente classificadas \cite{willett2013galaxy}. 

\textit{Rosseta@Home} outro importante projeto de ciência cidadã. Quando criado em 2005, este projeto seguia a mesma tendência do \textit{Seti@Home} assim como os demais projetos da família \textit{@Home}, ser mais uma computação voluntária. O objetivo deste projeto é de utilizar o grande poder computacional reunido para prever o enovelamento de proteínas, um processo químico em que a estrutura de uma proteína assume a sua configuração funcional, podendo assim desenvolver novas proteínas para combater diversas doenças. Como os demais projetos da família, os voluntários tinham que executar o instalador do projeto utilizando o BOINC e este só iria entrar em funcionamento quando o computador estivesse ocioso, em modo de protetor de tela. Porém, diversos emails foram enviados ao projeto de voluntários que queriam contribuir mais, dizendo ser possível obter melhores resultados se eles pudessem interagir com o modelo de proteína que estava aparecendo ali no protetor de tela. 

Com isto em mente, David Baker desenvolveu um novo projeto de pensamento voluntário com características de um jogo através da internet, onde os voluntários são os jogadores e precisão achar as melhores soluções para os desafios, utilizando o enovelamento de proteínas \cite{Hand2010}. \textit{Foldit}, lançado em 2008, provou que os voluntários conseguem realizar um melhor trabalho do que um computador. Este é um dos primeiros trabalhos que envolve tanto computação voluntária quanto o pensamento voluntário \cite{Cooper2010}.

Um dos resultados mais significativos, foi alcançado pelos jogadores do projeto \textit{Foldit} em 2011, onde os voluntários resolveram um problema proposto pelos pesquisadores ``jogando'' em apenas 3 semanas. Os cientistas lançaram o desafio a partir do instante em que os métodos automáticos começaram a não retornar bons resultados, ao analisar os resultados dos jogadores, observaram que estes eram suficientemente bons para encontrar uma rápida solução \cite{Khatib2011}.

\citeonline{mcgonigal2011reality} sugere que os problemas atuais poderiam ser solucionados efetivamente de outra forma, através dos jogos. Concientizando pessoas dos problemas reais e inserindo os diversos problemas como contexto dos jogos, os jogadores iriam buscar soluções para estes desafios. Diversos pesquisadores sugerem que construir projetos de ciência com a temática na forma de jogos poderiam atrair ainda mais os voluntários e resolver ainda mais rápido os desafios empregados \cite{Silvertown2009,Hand2010,Khatib2011,Anderson1999,Pereira:2013:NoInUs,Cooper2010,Mansell2012}.

\subsection{A Tecnológica Móvel}

As duas formas de ciência cidadã discutidas anteriormente tem suas vantagens conforme a necessidade do método de pesquisa científica que irá ser executada. Computação voluntária tem maior vantagem para as atividades científicas que requerem grande poder computacional, avaliando grandes quantidades de dados, o que levariam horas para uma pessoa realizar. Já o pensamento voluntário, mostra-se mais eficaz em atividades que requerem o poder cognitivo dos voluntários, como detecções de padrões utilizando inspeção visual.

Algumas vezes, os dados disponíveis para projetos científicos não são suficientes e a coleta de outros tipos de medidas se faz necessária. Adquirir novas fontes de dados não é um simples trabalho, necessita-se de equipes capazes de efetuar as coletas científicas conforme padrões exigidos e ferramentas adequadas para tal.
A realização desta tarefa, porém, se feita apenas pelos cientistas envolvidos diretamente com o projeto, pode se tornar custosa e demorada. Portanto, uma alternativa viável é a utilização de sensoriamento voluntário onde voluntários contribuem com dados, sejam anotações de observações, captação de imagem ou sons de um ambiente. A tecnologia atual permite soluções de menor custo para coletar diversos tipos de dados para serem utilizados em projetos científicos.

Em recentes relatórios, \citeonline{Gartner2014} aponta que em 2015 os dispositivos móveis (\textit{tablets}, celulares e \textit{smartphones}) irão ultrapassar os computadores pessoais (computadores de mesa e portáteis), em quantidade, conforme figura \ref{fig:gartner}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\textwidth]{figuras/gartner2014.png}
	\caption{Relatório Gartner julho/2014}
	\FONTE{\citeonline{Gartner2014}}
	\label{fig:gartner}
\end{figure}

A era pós-PC\footnote{A era \textit{post-PC} ou \textit{PC-plus} foi cuminada por ``FULANO'' em que estimava que os computadores deixariam de ser o principal aparato eletrônico.}, está se tornando realidade, os computadores pessoais estão sendo descentralizados como a décadas muitos já previam. Sua utilidade está mais fardada a um \textit{hub} tecnológico, servindo de meio de comunicação com outros dispositivos móveis como \textit{smartphones} e \textit{tablets}. No início dos anos 90, os celulares eram realidades para pouco, mas diversos fatores alteraram a rota deste para a sua popularização.

O avanço da tecnologia permitiu reduzir o tamanho dos sensores físicos e a quantidade de energia requerida por esses. Diversos tipos de sensores estão cada vez mais presentes em celulares, tornando dispositivos móveis em unidades de coleta de informação de grande precisão. Câmeras e sensores de localização já são frequentes na maioria dos celulares, estes permitem obter a sua localização em qualquer parte do globo terrestre através de \textit{GPS}\footnote{\textbf{G}lobal \textbf{P}ositioning \textbf{S}ystem é um sistema de navegação baseado em uma constelação de 24 satélites.}, sendo alguns modernos integrado com \textit{GLONASS}\footnote{\textbf{GLO}bal \textbf{NA}vigatsionnaya \textbf{S}putnikovaya \textbf{S}istema, um sistema de navegação russo, constituído por uma constelação de 21 satélites}. Há também dispositivos que apresentam sensores de proximidade, giroscópio, magenetômetro, acelerômetro, barômetro e de luz ambiente. Mas sempre há um sensor presente nos celulares, o microfone que em conjunto com os demais sensores supracitados torna-se uma poderosa ferramenta de monitoramento ambiental.






\subsection{Definição}
\subsection{Histórico}
\subsection{Avanços na Ciência}
\subsection{Suas Formas}
\subsubsection{Projetos}

\section{Sistemas de Monitoramento Ambientais Colaborativos}
\subsection{Projetos}

\section{ForestWatchers}
\subsection{Proposta}
\subsection{Metodologia}
\subsection{Resultados}

\section{Aplicativos Móveis}
\subsection{Surgimento}
\subsection{Divisão de Mercado}
\subsection{Aplicativos Nativos}
\subsection{Aplicativos Híbridos}



